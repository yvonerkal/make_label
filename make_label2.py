import streamlit as st
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import pandas as pd
import os
import io
import zipfile
from io import BytesIO
from PIL import Image
import uuid
from pypinyin import lazy_pinyin
from matplotlib.widgets import RectangleSelector
from matplotlib.patches import Rectangle

# ======== å·¥å…·å‡½æ•° =========
@st.cache_data(show_spinner=False)
def load_audio(file):
    return librosa.load(file, sr=None)

@st.cache_data(show_spinner=False)
def generate_spectrogram_image(y, sr):
    fig, ax = plt.subplots(figsize=(5, 3))
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax)
    ax.set(title="Spectrogram (dB)")
    fig.tight_layout()
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=200, bbox_inches="tight")
    buf.seek(0)
    plt.close(fig)
    return Image.open(buf)

@st.cache_data(show_spinner=False)
def generate_waveform_image(y, sr):
    fig, ax = plt.subplots(figsize=(5, 3))
    librosa.display.waveshow(y, sr=sr)
    ax.set(title="Waveform")
    fig.tight_layout()
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=200, bbox_inches="tight")
    buf.seek(0)
    plt.close(fig)
    return Image.open(buf)

def get_pinyin_abbr(text):
    return ''.join([p[0] for p in lazy_pinyin(text) if p])

def get_full_pinyin(text):
    return ''.join(lazy_pinyin(text))

# ======== äº¤äº’å¼ç”»æ¡†åŠŸèƒ½ =========
class BoxAnnotator:
    def __init__(self, fig, ax, sr):
        self.fig = fig
        self.ax = ax
        self.sr = sr
        self.boxes = []
        self.current_box = None
        self.rect_selector = RectangleSelector(
            ax, self.on_select,
            useblit=True,
            button=[1],  # å·¦é”®
            minspanx=5, minspany=5,
            spancoords='pixels',
            interactive=True
        )
        self.fig.canvas.mpl_connect('button_press_event', self.on_press)
        self.fig.canvas.mpl_connect('motion_notify_event', self.on_motion)
        self.fig.canvas.mpl_connect('button_release_event', self.on_release)
    
    def on_press(self, event):
        if event.inaxes == self.ax:
            self.start_point = (event.xdata, event.ydata)
            self.current_rect = Rectangle(
                (event.xdata, event.ydata), 0, 0,
                linewidth=2, edgecolor='r', facecolor='none'
            )
            self.ax.add_patch(self.current_rect)
    
    def on_motion(self, event):
        if hasattr(self, 'current_rect') and event.inaxes == self.ax:
            width = event.xdata - self.start_point[0]
            height = event.ydata - self.start_point[1]
            self.current_rect.set_width(width)
            self.current_rect.set_height(height)
            self.fig.canvas.draw()
    
    def on_release(self, event):
        if hasattr(self, 'current_rect') and event.inaxes == self.ax:
            self.current_box = {
                'start': min(self.start_point[0], event.xdata),
                'end': max(self.start_point[0], event.xdata),
                'low_freq': min(self.start_point[1], event.ydata),
                'high_freq': max(self.start_point[1], event.ydata)
            }
            self.current_rect.remove()
            del self.current_rect
    
    def on_select(self, eclick, erelease):
        pass  # ä¿ç•™åŽŸæœ‰æŽ¥å£
    
    def add_box(self, label):
        if self.current_box:
            self.current_box['label'] = label
            self.boxes.append(self.current_box)
            self.draw_boxes()
            self.current_box = None
            return True
        return False
    
    def draw_boxes(self):
        for box in self.boxes:
            rect = Rectangle(
                (box['start'], box['low_freq']),
                box['end'] - box['start'],
                box['high_freq'] - box['low_freq'],
                linewidth=2, edgecolor='b', facecolor='none'
            )
            self.ax.add_patch(rect)
            self.ax.text(
                box['start'], box['high_freq'], box['label'],
                color='white', backgroundcolor='red', fontsize=10
            )
    
    def remove_last(self):
        if self.boxes:
            self.boxes.pop()
            self.ax.clear()
            D = librosa.amplitude_to_db(np.abs(librosa.stft(self.y)), ref=np.max)
            librosa.display.specshow(D, sr=self.sr, x_axis='time', y_axis='log', ax=self.ax)
            self.draw_boxes()
            return True
        return False

# ======== Session çŠ¶æ€åˆå§‹åŒ– =========
if "dynamic_species_list" not in st.session_state:
    st.session_state["dynamic_species_list"] = []
if "current_selected_labels" not in st.session_state:
    st.session_state.current_selected_labels = set()
if "audio_state" not in st.session_state:
    st.session_state.audio_state = {
        "processed_files": set(),
        "current_index": 0,
        "segment_info": {},
        "last_audio_file": None,
        "last_seg_idx": -1,
        "annotations": []
    }
if "filtered_labels_cache" not in st.session_state:
    st.session_state.filtered_labels_cache = {}
if "annotation_mode" not in st.session_state:
    st.session_state.annotation_mode = "åˆ†æ®µæ ‡æ³¨"
if "box_annotator" not in st.session_state:
    st.session_state.box_annotator = None

st.set_page_config(layout="wide")
st.title("ðŸ¸ é’è›™éŸ³é¢‘æ ‡æ³¨å·¥å…·")

# ======== æ ‡ç­¾ç®¡ç†ç»„ä»¶ =========
def label_management_component():
    with st.sidebar:
        st.markdown("### ðŸ·ï¸ æ ‡ç­¾è®¾ç½®")
        with st.form("label_form", clear_on_submit=True):
            label_file = st.file_uploader("ä¸Šä¼ æ ‡ç­¾æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", type=["txt"], key="label_file")
            submit_label = st.form_submit_button("åŠ è½½æ ‡ç­¾")
            if submit_label and label_file:
                try:
                    species_list = [line.strip() for line in label_file.read().decode("utf-8").split("\n") if line.strip()]
                    if species_list:
                        st.session_state["dynamic_species_list"] = species_list
                        st.success(f"åŠ è½½æˆåŠŸï¼å…± {len(species_list)} ä¸ªæ ‡ç­¾")
                        st.rerun()
                    else:
                        st.error("æ ‡ç­¾æ–‡ä»¶ä¸ºç©º")
                except Exception as e:
                    st.error(f"é”™è¯¯ï¼š{str(e)}")
        st.markdown("#### å½“å‰æ ‡ç­¾é¢„è§ˆ")
        st.write(st.session_state["dynamic_species_list"][:5] + (
            ["..."] if len(st.session_state["dynamic_species_list"]) > 5 else []))
        
        # æ ‡æ³¨æ¨¡å¼é€‰æ‹©
        st.session_state.annotation_mode = st.radio(
            "æ ‡æ³¨æ¨¡å¼",
            ["åˆ†æ®µæ ‡æ³¨", "é¢‘è°±å›¾ç”»æ¡†"],
            index=0 if st.session_state.annotation_mode == "åˆ†æ®µæ ‡æ³¨" else 1
        )
    return st.session_state["dynamic_species_list"]

# ======== å³ä¾§æ ‡æ³¨æ ‡ç­¾ç»„ä»¶ =========
def annotation_labels_component(current_segment_key):
    species_list = st.session_state["dynamic_species_list"]
    col_labels = st.container()

    with col_labels:
        st.markdown("### ç‰©ç§æ ‡ç­¾ï¼ˆå¯å¤šé€‰ï¼‰")
        if not species_list:
            st.warning("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ ‡ç­¾æ–‡ä»¶")
            return None, None

        # æœç´¢æ¡†
        search_query = st.text_input("ðŸ” æœç´¢æ ‡ç­¾ï¼ˆæ”¯æŒä¸­æ–‡ã€æ‹¼éŸ³é¦–å­—æ¯ã€å…¨æ‹¼ï¼‰", "", key=f"search_{current_segment_key}")

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{current_segment_key}_{search_query}"

        # ç¼“å­˜æœç´¢ç»“æžœ
        if cache_key not in st.session_state.filtered_labels_cache:
            filtered_species = []
            if search_query:
                search_lower = search_query.lower()
                for label in species_list:
                    label_lower = label.lower()
                    if (search_lower in label_lower or
                            search_lower in get_pinyin_abbr(label) or
                            search_lower in get_full_pinyin(label)):
                        filtered_species.append(label)
            else:
                filtered_species = species_list.copy()
            st.session_state.filtered_labels_cache[cache_key] = filtered_species

        filtered_species = st.session_state.filtered_labels_cache[cache_key]

        # æ˜¾ç¤ºæ ‡ç­¾é€‰æ‹©æ¡†
        for label in filtered_species:
            key = f"label_{label}_{current_segment_key}"
            is_selected = label in st.session_state.current_selected_labels
            if st.checkbox(label, key=key, value=is_selected):
                st.session_state.current_selected_labels.add(label)
            else:
                st.session_state.current_selected_labels.discard(label)

        st.markdown("### å·²é€‰æ ‡ç­¾")
        st.info(f"å·²é€‰æ•°é‡ï¼š{len(st.session_state.current_selected_labels)}")
        
        # æ“ä½œæŒ‰é’®
        col_save, col_skip = st.columns(2)
        return col_save, col_skip

# ======== é¢‘è°±å›¾ç”»æ¡†ç»„ä»¶ ========
def spectral_annotation_component(y, sr, current_segment_key):
    col_main, col_labels = st.columns([3, 1])
    
    with col_main:
        st.subheader("ðŸŽ§ é¢‘è°±å›¾ç”»æ¡†æ ‡æ³¨")
        
        # åˆ›å»ºé¢‘è°±å›¾
        fig, ax = plt.subplots(figsize=(10, 4))
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax)
        
        # åˆå§‹åŒ–æˆ–èŽ·å–ç”»æ¡†å·¥å…·
        if st.session_state.box_annotator is None:
            st.session_state.box_annotator = BoxAnnotator(fig, ax, sr)
            st.session_state.box_annotator.y = y  # ä¿å­˜éŸ³é¢‘æ•°æ®
        else:
            st.session_state.box_annotator.draw_boxes()
        
        st.pyplot(fig)
        
        # éŸ³é¢‘æ’­æ”¾å™¨
        audio_bytes = BytesIO()
        sf.write(audio_bytes, y, sr, format='WAV')
        st.audio(audio_bytes, format="audio/wav")
        
        # ç”»æ¡†æ“ä½œæŒ‰é’®
        if st.button("ç¡®è®¤å½“å‰æ¡†é€‰åŒºåŸŸ"):
            if st.session_state.current_selected_labels:
                label = list(st.session_state.current_selected_labels)[0]  # å–ç¬¬ä¸€ä¸ªé€‰ä¸­çš„æ ‡ç­¾
                if st.session_state.box_annotator.add_box(label):
                    st.success("æ ‡æ³¨æ¡†å·²æ·»åŠ ï¼")
                    st.rerun()
                else:
                    st.warning("è¯·å…ˆç”¨é¼ æ ‡åœ¨é¢‘è°±å›¾ä¸Šæ¡†é€‰åŒºåŸŸ")
            else:
                st.warning("è¯·å…ˆåœ¨å³ä¾§é€‰æ‹©æ ‡ç­¾")
        
        if st.button("æ’¤é”€ä¸Šä¸€ä¸ªæ ‡æ³¨æ¡†"):
            if st.session_state.box_annotator.remove_last():
                st.rerun()
    
    with col_labels:
        # ä½¿ç”¨åŽŸæœ‰çš„æ ‡ç­¾é€‰æ‹©ç»„ä»¶
        col_save, col_skip = annotation_labels_component(current_segment_key)
        
        if st.session_state.box_annotator and st.session_state.box_annotator.boxes:
            st.markdown("### å½“å‰æ ‡æ³¨æ¡†")
            for i, box in enumerate(st.session_state.box_annotator.boxes):
                st.write(f"{i+1}. {box['label']} ({box['start']:.2f}s-{box['end']:.2f}s, {box['low_freq']:.0f}-{box['high_freq']:.0f}Hz)")
        
        if col_save and st.button("ä¿å­˜æ‰€æœ‰æ ‡æ³¨", key=f"save_boxes_{current_segment_key}"):
            return True
    
    return False

# ======== éŸ³é¢‘å¤„ç†ä¸»é€»è¾‘ ========
def process_audio():
    audio_state = st.session_state.audio_state
    output_dir = "uploaded_audios"
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "annotations.csv")

    try:
        df_old = pd.read_csv(csv_path) if os.path.exists(csv_path) else pd.DataFrame(
            columns=["filename", "segment_index", "start_time", "end_time", "labels", "low_freq", "high_freq"]
        )
    except:
        df_old = pd.DataFrame(columns=["filename", "segment_index", "start_time", "end_time", "labels", "low_freq", "high_freq"])

    with st.sidebar:
        st.markdown("### ðŸŽµ éŸ³é¢‘ä¸Šä¼ ")
        uploaded_files = st.file_uploader("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ (.wav)", type=["wav"], accept_multiple_files=True, key="audio_files")
        st.markdown("### ðŸ“¥ ä¸‹è½½ç»“æžœ")
        if os.path.exists(csv_path):
            with open(csv_path, "rb") as f:
                st.download_button("ðŸ“„ ä¸‹è½½CSV", f, "annotations.csv", "text/csv")
        annotated_paths = []
        if os.path.exists(csv_path) and "segment_index" in pd.read_csv(csv_path).columns:
            for _, row in pd.read_csv(csv_path).iterrows():
                try:
                    fname = str(row["segment_index"])
                    if fname and os.path.exists(os.path.join(output_dir, fname)):
                        annotated_paths.append(os.path.join(output_dir, fname))
                except:
                    pass
        if annotated_paths:
            with zipfile.ZipFile(zip_buf := BytesIO(), "w") as zf:
                for p in annotated_paths:
                    zf.write(p, os.path.basename(p))
            zip_buf.seek(0)
            st.download_button("ðŸŽµ ä¸‹è½½éŸ³é¢‘ç‰‡æ®µ", zip_buf, "annotated_segments.zip", "application/zip")

    if not uploaded_files:
        st.info("è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
        return

    unprocessed = [f for f in uploaded_files if not (audio_state["segment_info"].get(f.name) and
                                                     audio_state["segment_info"][f.name]["current_seg"] >=
                                                     audio_state["segment_info"][f.name]["total_seg"]]

    if audio_state["current_index"] < len(unprocessed):
        audio_file = unprocessed[audio_state["current_index"]]
        y, sr = load_audio(audio_file)
        total_duration = librosa.get_duration(y=y, sr=sr)
        total_segments = int(np.ceil(total_duration / 5.0))
        seg_idx = audio_state["segment_info"].get(audio_file.name, {"current_seg": 0})["current_seg"]
        current_segment_key = f"{audio_file.name}_{seg_idx}"

        if (audio_state["last_audio_file"] != audio_file.name or audio_state["last_seg_idx"] != seg_idx):
            st.session_state.current_selected_labels = set()
            st.session_state.box_annotator = None
            audio_state["last_audio_file"], audio_state["last_seg_idx"] = audio_file.name, seg_idx

        st.header(f"æ ‡æ³¨éŸ³é¢‘: {audio_file.name} - ç¬¬ {seg_idx + 1}/{total_segments} æ®µ")
        
        start_sec, end_sec = seg_idx * 5.0, min((seg_idx + 1) * 5.0, total_duration)
        segment_y = y[int(start_sec * sr):int(end_sec * sr)]

        if st.session_state.annotation_mode == "é¢‘è°±å›¾ç”»æ¡†":
            if spectral_annotation_component(segment_y, sr, current_segment_key):
                save_spectral_annotations(audio_file, seg_idx, start_sec, end_sec, segment_y, sr, output_dir)
        else:
            col_main, col_labels = st.columns([3, 1])
            
            with col_main:
                st.subheader("ðŸŽ§ æ’­æ”¾å½“å‰ç‰‡æ®µ")
                audio_bytes = BytesIO()
                sf.write(audio_bytes, segment_y, sr, format='WAV')
                st.audio(audio_bytes, format="audio/wav")

                col1, col2 = st.columns(2)
                with col1:
                    st.image(generate_waveform_image(segment_y, sr), caption="æ³¢å½¢å›¾", use_container_width=True)
                with col2:
                    st.image(generate_spectrogram_image(segment_y, sr), caption="é¢‘è°±å›¾", use_container_width=True)

            with col_labels:
                col_save, col_skip = annotation_labels_component(current_segment_key)

                if col_save and st.button("ä¿å­˜æœ¬æ®µæ ‡æ³¨", key=f"save_{current_segment_key}"):
                    save_segment_annotation(audio_file, seg_idx, start_sec, end_sec, segment_y, sr, output_dir)
                
                if col_skip and st.button("è·³è¿‡æœ¬æ®µ", key=f"skip_{current_segment_key}"):
                    if seg_idx + 1 < total_segments:
                        audio_state["segment_info"][audio_file.name]["current_seg"] += 1
                    else:
                        audio_state["processed_files"].add(audio_file.name)
                        audio_state["current_index"] += 1
                    st.rerun()

    else:
        st.success("ðŸŽ‰ æ‰€æœ‰éŸ³é¢‘æ ‡æ³¨å®Œæˆï¼")

    st.session_state.audio_state = audio_state

# ======== ä¿å­˜å‡½æ•° ========
def save_segment_annotation(audio_file, seg_idx, start_sec, end_sec, segment_y, sr, output_dir):
    csv_path = os.path.join(output_dir, "annotations.csv")
    
    try:
        if not st.session_state.current_selected_labels:
            st.warning("â—è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ ‡ç­¾")
            return

        base_name = os.path.splitext(audio_file.name)[0]
        try:
            base_name = base_name.encode('utf-8').decode('utf-8')
        except:
            base_name = "audio_segment"

        unique_id = uuid.uuid4().hex[:8]
        segment_filename = f"{base_name}_seg{seg_idx}_{unique_id}.wav"
        segment_path = os.path.join(output_dir, segment_filename)

        with sf.SoundFile(segment_path, 'w', samplerate=sr, channels=1) as f:
            f.write(segment_y)

        clean_labels = [label.replace("/", "").replace("\\", "") for label in st.session_state.current_selected_labels]
        entry = {
            "filename": audio_file.name,
            "segment_index": segment_filename,
            "start_time": round(start_sec, 3),
            "end_time": round(end_sec, 3),
            "labels": ",".join(clean_labels),
            "low_freq": None,
            "high_freq": None
        }

        new_df = pd.DataFrame([entry])
        if os.path.exists(csv_path):
            existing_df = pd.read_csv(csv_path)
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df

        combined_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        audio_state = st.session_state.audio_state
        if audio_file.name not in audio_state["segment_info"]:
            audio_state["segment_info"][audio_file.name] = {
                "current_seg": 0,
                "total_seg": int(np.ceil(librosa.get_duration(y=segment_y, sr=sr) / 5.0))
            }

        if seg_idx + 1 < audio_state["segment_info"][audio_file.name]["total_seg"]:
            audio_state["segment_info"][audio_file.name]["current_seg"] += 1
        else:
            audio_state["processed_files"].add(audio_file.name)
            audio_state["current_index"] += 1

        st.session_state.audio_state = audio_state
        st.session_state.current_selected_labels = set()
        st.success(f"æˆåŠŸä¿å­˜æ ‡æ³¨ï¼æ–‡ä»¶: {segment_filename}")
        st.balloons()
        st.rerun()

    except Exception as e:
        st.error(f"ä¿å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def save_spectral_annotations(audio_file, seg_idx, segment_start, segment_end, segment_y, sr, output_dir):
    csv_path = os.path.join(output_dir, "annotations.csv")
    
    try:
        if not st.session_state.box_annotator or not st.session_state.box_annotator.boxes:
            st.warning("è¯·è‡³å°‘æ·»åŠ ä¸€ä¸ªæ ‡æ³¨æ¡†")
            return

        base_name = os.path.splitext(audio_file.name)[0]
        try:
            base_name = base_name.encode('utf-8').decode('utf-8')
        except:
            base_name = "audio_segment"

        entries = []
        for i, box in enumerate(st.session_state.box_annotator.boxes):
            # è®¡ç®—å®žé™…æ—¶é—´ä½ç½®
            abs_start = segment_start + box['start']
            abs_end = segment_start + box['end']
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            unique_id = uuid.uuid4().hex[:8]
            segment_filename = f"{base_name}_seg{seg_idx}_box{i}_{unique_id}.wav"
            segment_path = os.path.join(output_dir, segment_filename)
            
            # æˆªå–å¯¹åº”æ—¶é—´æ®µçš„éŸ³é¢‘
            start_sample = int(box['start'] * sr)
            end_sample = int(box['end'] * sr)
            box_audio = segment_y[start_sample:end_sample]
            
            # ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
            with sf.SoundFile(segment_path, 'w', samplerate=sr, channels=1) as f:
                f.write(box_audio)
            
            # åˆ›å»ºè®°å½•
            entries.append({
                "filename": audio_file.name,
                "segment_index": segment_filename,
                "start_time": round(abs_start, 3),
                "end_time": round(abs_end, 3),
                "labels": box['label'],
                "low_freq": box['low_freq'],
                "high_freq": box['high_freq']
            })

        # ä¿å­˜åˆ°CSV
        new_df = pd.DataFrame(entries)
        if os.path.exists(csv_path):
            existing_df = pd.read_csv(csv_path)
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df

        combined_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # æ›´æ–°çŠ¶æ€
        audio_state = st.session_state.audio_state
        if seg_idx + 1 < audio_state["segment_info"][audio_file.name]["total_seg"]:
            audio_state["segment_info"][audio_file.name]["current_seg"] += 1
        else:
            audio_state["processed_files"].add(audio_file.name)
            audio_state["current_index"] += 1

        st.session_state.audio_state = audio_state
        st.session_state.box_annotator = None
        st.session_state.current_selected_labels = set()
        st.success(f"æˆåŠŸä¿å­˜ {len(entries)} ä¸ªæ ‡æ³¨æ¡†ï¼")
        st.balloons()
        st.rerun()

    except Exception as e:
        st.error(f"ä¿å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    label_management_component()
    process_audio()
