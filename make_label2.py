# åœ¨åŽŸæœ‰ä»£ç åŸºç¡€ä¸Šæ–°å¢žä»¥ä¸‹å†…å®¹

# ======== æ–°å¢žç”»æ¡†æ ‡æ³¨ç›¸å…³å‡½æ•° =========
@st.cache_data(show_spinner=False)
def generate_interactive_spectrogram(y, sr):
    """ç”Ÿæˆäº¤äº’å¼é¢‘è°±å›¾"""
    fig, ax = plt.subplots(figsize=(10, 4))
    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax)
    ax.set(title="Spectrogram (Click and drag to annotate)")
    return fig, ax

def draw_rectangle(fig, ax, start_time, end_time, low_freq, high_freq, label=None):
    """åœ¨é¢‘è°±å›¾ä¸Šç»˜åˆ¶çŸ©å½¢æ¡†"""
    width = end_time - start_time
    height = high_freq - low_freq
    rect = plt.Rectangle((start_time, low_freq), width, height,
                        linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)
    if label:
        ax.text(start_time, high_freq, label, color='white', 
               backgroundcolor='red', fontsize=10)
    return fig

# ======== ä¿®æ”¹éŸ³é¢‘å¤„ç†é€»è¾‘ =========
def process_audio():
    audio_state = st.session_state.audio_state
    output_dir = "uploaded_audios"
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "annotations.csv")

    # æ–°å¢žæ ‡æ³¨æ¨¡å¼åˆ‡æ¢
    annotation_mode = st.sidebar.radio("æ ‡æ³¨æ¨¡å¼", ["åˆ†æ®µæ ‡æ³¨", "é¢‘è°±å›¾ç”»æ¡†"], index=0)
    
    # ... (ä¿ç•™åŽŸæœ‰çš„æ–‡ä»¶ä¸Šä¼ å’Œä¸‹è½½é€»è¾‘)

    if not uploaded_files:
        st.info("è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
        return

    unprocessed = [f for f in uploaded_files if not (audio_state["segment_info"].get(f.name) and
                                                     audio_state["segment_info"][f.name]["current_seg"] >=
                                                     audio_state["segment_info"][f.name]["total_seg"])]

    if audio_state["current_index"] < len(unprocessed):
        audio_file = unprocessed[audio_state["current_index"]]
        y, sr = load_audio(audio_file)
        total_duration = librosa.get_duration(y=y, sr=sr)
        total_segments = int(np.ceil(total_duration / 5.0))
        seg_idx = audio_state["segment_info"].get(audio_file.name, {"current_seg": 0})["current_seg"]
        current_segment_key = f"{audio_file.name}_{seg_idx}"

        if (audio_state["last_audio_file"] != audio_file.name or audio_state["last_seg_idx"] != seg_idx):
            st.session_state.current_selected_labels = set()
            audio_state["last_audio_file"], audio_state["last_seg_idx"] = audio_file.name, seg_idx
            # åˆå§‹åŒ–ç”»æ¡†æ ‡æ³¨çŠ¶æ€
            if "boxes" not in st.session_state:
                st.session_state.boxes = []

        st.header(f"æ ‡æ³¨éŸ³é¢‘: {audio_file.name} - ç¬¬ {seg_idx + 1}/{total_segments} æ®µ")
        
        if annotation_mode == "é¢‘è°±å›¾ç”»æ¡†":
            process_spectral_annotation(y, sr, audio_file, seg_idx, output_dir)
        else:
            process_segment_annotation(y, sr, audio_file, seg_idx, total_duration, current_segment_key, output_dir)

    else:
        st.success("ðŸŽ‰ æ‰€æœ‰éŸ³é¢‘æ ‡æ³¨å®Œæˆï¼")

    st.session_state.audio_state = audio_state

def process_spectral_annotation(y, sr, audio_file, seg_idx, output_dir):
    """å¤„ç†é¢‘è°±å›¾ç”»æ¡†æ ‡æ³¨"""
    col_main, col_labels = st.columns([3, 1])
    
    with col_main:
        st.subheader("ðŸŽ§ é¢‘è°±å›¾ç”»æ¡†æ ‡æ³¨")
        start_sec, end_sec = seg_idx * 5.0, min((seg_idx + 1) * 5.0, librosa.get_duration(y=y, sr=sr))
        segment_y = y[int(start_sec * sr):int(end_sec * sr)]
        
        # ç”Ÿæˆäº¤äº’å¼é¢‘è°±å›¾
        fig, ax = generate_interactive_spectrogram(segment_y, sr)
        
        # æ˜¾ç¤ºå·²æœ‰æ ‡æ³¨æ¡†
        for box in st.session_state.get("boxes", []):
            fig = draw_rectangle(fig, ax, box["start_time"], box["end_time"], 
                               box["low_freq"], box["high_freq"], box["label"])
        
        # æ˜¾ç¤ºé¢‘è°±å›¾
        st.pyplot(fig)
        
        # éŸ³é¢‘æ’­æ”¾å™¨
        audio_bytes = BytesIO()
        sf.write(audio_bytes, segment_y, sr, format='WAV')
        st.audio(audio_bytes, format="audio/wav")
        
        # ç”»æ¡†æŽ§ä»¶
        with st.form("box_form"):
            st.markdown("### æ·»åŠ æ ‡æ³¨æ¡†")
            col1, col2 = st.columns(2)
            with col1:
                start_time = st.number_input("å¼€å§‹æ—¶é—´(s)", min_value=0.0, max_value=5.0, step=0.1)
                end_time = st.number_input("ç»“æŸæ—¶é—´(s)", min_value=0.0, max_value=5.0, value=1.0, step=0.1)
            with col2:
                low_freq = st.number_input("æœ€ä½Žé¢‘çŽ‡(Hz)", min_value=0, max_value=sr//2, value=1000, step=100)
                high_freq = st.number_input("æœ€é«˜é¢‘çŽ‡(Hz)", min_value=0, max_value=sr//2, value=3000, step=100)
            
            if st.form_submit_button("æ·»åŠ æ ‡æ³¨æ¡†"):
                if end_time <= start_time:
                    st.error("ç»“æŸæ—¶é—´å¿…é¡»å¤§äºŽå¼€å§‹æ—¶é—´")
                elif high_freq <= low_freq:
                    st.error("æœ€é«˜é¢‘çŽ‡å¿…é¡»å¤§äºŽæœ€ä½Žé¢‘çŽ‡")
                else:
                    st.session_state.boxes.append({
                        "start_time": start_time,
                        "end_time": end_time,
                        "low_freq": low_freq,
                        "high_freq": high_freq,
                        "label": ""
                    })
                    st.rerun()
        
        # æ’¤é”€æŒ‰é’®
        if st.button("æ’¤é”€ä¸Šä¸€ä¸ªæ ‡æ³¨æ¡†") and st.session_state.boxes:
            st.session_state.boxes.pop()
            st.rerun()
    
    with col_labels:
        # ä¸ºæ¯ä¸ªæ¡†é€‰æ‹©æ ‡ç­¾
        if st.session_state.boxes:
            st.markdown("### æ ‡æ³¨æ¡†æ ‡ç­¾")
            for i, box in enumerate(st.session_state.boxes):
                if not box["label"]:
                    label = st.selectbox(
                        f"æ ‡æ³¨æ¡†{i+1} ({box['start_time']:.1f}s-{box['end_time']:.1f}s)",
                        st.session_state["dynamic_species_list"],
                        key=f"box_label_{i}"
                    )
                    st.session_state.boxes[i]["label"] = label
            
            # ä¿å­˜æ‰€æœ‰æ ‡æ³¨
            if st.button("ä¿å­˜æ‰€æœ‰æ ‡æ³¨"):
                save_spectral_annotations(audio_file, seg_idx, start_sec, output_dir)
        else:
            st.info("è¯·å…ˆæ·»åŠ æ ‡æ³¨æ¡†")

def save_spectral_annotations(audio_file, seg_idx, start_sec, output_dir):
    """ä¿å­˜é¢‘è°±å›¾æ ‡æ³¨ç»“æžœ"""
    try:
        # ä¸ºæ¯ä¸ªæ ‡æ³¨æ¡†ä¿å­˜å•ç‹¬çš„éŸ³é¢‘ç‰‡æ®µ
        for i, box in enumerate(st.session_state.boxes):
            if not box["label"]:
                st.error(f"è¯·ä¸ºæ ‡æ³¨æ¡†{i+1}é€‰æ‹©æ ‡ç­¾")
                return
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            unique_id = uuid.uuid4().hex[:8]
            segment_filename = f"{os.path.splitext(audio_file.name)[0]}_seg{seg_idx}_box{i}_{unique_id}.wav"
            
            # ä¿å­˜æ ‡æ³¨ä¿¡æ¯åˆ°CSV
            entry = {
                "filename": audio_file.name,
                "segment_index": segment_filename,
                "start_time": round(start_sec + box["start_time"], 3),
                "end_time": round(start_sec + box["end_time"], 3),
                "low_freq": box["low_freq"],
                "high_freq": box["high_freq"],
                "labels": box["label"]
            }
            
            # æ·»åŠ åˆ°CSVæ–‡ä»¶
            csv_path = os.path.join(output_dir, "annotations.csv")
            df = pd.DataFrame([entry])
            if os.path.exists(csv_path):
                df.to_csv(csv_path, mode='a', header=False, index=False)
            else:
                df.to_csv(csv_path, index=False)
        
        st.success("æ ‡æ³¨ä¿å­˜æˆåŠŸï¼")
        st.session_state.boxes = []  # æ¸…ç©ºå½“å‰æ ‡æ³¨
        st.rerun()
    
    except Exception as e:
        st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")

# ... (ä¿ç•™åŽŸæœ‰çš„process_segment_annotationå‡½æ•°å’Œå…¶ä»–ä»£ç )
